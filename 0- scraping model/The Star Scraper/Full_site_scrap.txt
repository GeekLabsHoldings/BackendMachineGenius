const puppeteer = require('puppeteer');

const getURLs = async () => {
  const browser = await puppeteer.launch({
    headless: false,
    defaultViewport: null,
  });

  const page = await browser.newPage();

  // Navigate to the initial page to scrape URLs
  await page.goto("https://www.thestar.com/politics/", {
    waitUntil: "domcontentloaded",
  });

  // Scrape URLs from the initial page
  const URLs = await page.evaluate(() => {
    const ScrapeList = document.querySelectorAll(".tnt-has-block-bg a.tnt-asset-link");
    return Array.from(ScrapeList).map((Scrape) => {
      let href = Scrape.getAttribute("href");
      if (!href.startsWith('http')) {
        href = `https://www.thestar.com${href}`;
      }
      return href;
    });
  });

  console.log('Scraped URLs:', URLs);

  // Close the initial page
  await page.close();

  // Loop through each URL and scrape content
  for (let i = 0; i < URLs.length; i++) {
    const url = URLs[i];
    console.log(`Scraping content from: ${url}`);

    // Open a new page for each URL
    const contentPage = await browser.newPage();
    await contentPage.goto(url, {
      waitUntil: "domcontentloaded",
    });

    // Scrape content from the page
    const content = await contentPage.evaluate(() => {
      const ScrapeList = document.querySelectorAll(".asset-content p");
      return Array.from(ScrapeList).map((Scrape) => Scrape.innerText);
    });

    console.log(`Content from ${url}:`);
    console.log(content);
    console.log('---------------------');

    // Close the content page after scraping
    await contentPage.close();
  }

  // Close the browser
  await browser.close();
};

// Start the scraping process
getURLs();
